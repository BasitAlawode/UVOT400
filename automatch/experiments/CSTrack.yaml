COMMON:

MODEL:
  Name: CSTrack

  # parameters
  nc: 1  # number of classes
  id_embedding: 512 # id embedding channels
  DetectorType: 'YOLOv5'
  AssociationType: 'DeepSort'

  Detector:
    YOLOv5:
      depth_multiple: 1.0  # model depth multiple
      width_multiple: 1.0  # layer channel multiple

      # anchors
      anchors:
        - [8,24, 11,34, 16,48]  # P3/8
        - [32,96, 45,135, 64,192]  # P4/16
        - [128,384, 180,540, 256,640]  # P5/32

      # YOLOv5 backbone
      backbone:
        # [from, number, module, args]
        [[-1, 1, Focus, [64, 3]],  # 0-P1/2
         [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
         [-1, 3, BottleneckCSP, [128]],
         [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
         [-1, 9, BottleneckCSP, [256]],
         [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
         [-1, 9, BottleneckCSP, [512]],
         [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
         [-1, 1, SPP, [1024, [5, 9, 13]]],
         [-1, 3, BottleneckCSP, [1024, False]],  # 9
        ]

      # YOLOv5 head
      head:
        [[-1, 1, Conv, [512, 1, 1]],
         [-1, 1, nn.Upsample, [None, 2, 'nearest']],
         [[-1, 6], 1, Concat, [1]],  # cat backbone P4
         [-1, 3, BottleneckCSP, [512, False]],  # 13

         [-1, 1, Conv, [256, 1, 1]],
         [-1, 1, nn.Upsample, [None, 2, 'nearest']],
         [[-1, 4], 1, Concat, [1]],  # cat backbone P3
         [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)

         [-1, 1, Conv, [256, 3, 2]],
         [[-1, 14], 1, Concat, [1]],  # cat head P4
         [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)

         [-1, 1, Conv, [512, 3, 2]],
         [[-1, 10], 1, Concat, [1]],  # cat head P5
         [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)

         [17, 1, CCN, [9,256]],  # CCN
         [20, 1, CCN, [9,512]],  # CCN
         [23, 1, CCN, [9,1024]],  # CCN
         [[24,25,26], 1, SAAN, [id_embedding]],  # SAAN
         [[24,25,26], 1, Detect, [nc, anchors, 0]],  # Detect(P3, P4, P5)
        ]


TRAIN:
  RESUME: False                        # False or 'last' or ckpt path
  BATCH_SIZE: 8                        # [total] batch size for all gpus  / default: 10
  UPDATE_BATCH: 64                     # how many images corresponds to once loss backward
  START_EPOCH: 0
  END_EPOCH: 30
  IMG_SIZE: !!python/tuple [1088, 608]  # input image size
  PRETRAIN: 'yolov5l.pt'                # in 'pretrain' dir
                                        # if resume, directly set PRETRAIN to corresponding weights

  ADAM: False                           # bool: use adam optimizer
  SYNCBN: False                         # use sync-bn
  LR_G0: 0.005                          # (lr0) lr for group 0 parameters (SGD=1E-2, Adam=1E-3)
  MOMENTUM: 0.95                        # SGD momentum/Adam beta1
  WEIGHT_DECAY: 0.0005                  # optimizer weight decay 5e-4
  SYNC_BN: False                        # sync bn
  PLOT_LR: False                         # plot learning rate curves

  CLS_WEIGHT: 0.5                       # (cls) cls loss gain  (default: 80 cls, each is 0.5/80)
  CLS_P_WEIGHT: 1.0                     # cls BCELoss positive_weight
  OBJ_WEIGHT: 1.0                              # (obj) obj loss gain (scale with pixels)
  OBJ_P_WEIGHT: 1.0                     # obj BCELoss positive_weight

  GIOU_WEIGHT: 0.05                            # GIoU loss gain
  IOU_THRESH: 0.20                      # (iou_t) IoU training threshold
  ANCHOR_THRESH: 4.0                    # (anchor_t) anchor-multiple threshold, for anchor assign
  FL_GAMMA: 0.0                         # (fl_gamma) focal loss gamma (efficientDet default gamma=1.5)

  FINETUNE: False                       # finetuning on MOT benchamrks
  RECT_TRAIN: False                     # bool: rectangular training
  FIX_LAYERS: []
  UNFIX_EPOCH: None

  WANDB_ONLINE: True

  DATASET:
    ROOT_DIR: '/datassd/tracking/MOT/LCFORMAT'
    WHICH_MODE: 'Default'  # (see CONFIG)

    CONFIG:
      # includes: mot15_train, mot15_val, mot17_train, mot17_val, mot17_half_train, mot17_half_val
      #           caltech_all, caltech_train, caltech_val, citypersons_train, citypersons_val,
      #           crowdhuman_train, crowdhuman_val, cuhksysu_train, cuhksysu_val, eth_train,
      #           prw_train, prw_val, panda_train, panda_val
      # see lib/dataset/
      Default:   # reproduce results in CSTrack/OMC (data_ch.json in OMC codes)...
        TRAIN_USE: ['mot17_train', 'caltech_train', 'citypersons_train', 'cuhksysu_train',
                  'prw_train', 'eth_train', 'crowdhuman_train', 'crowdhuman_val']
        VAL_USE: ['mot15_val']

      OnlyMOT17:
        TRAIN_USE: ['mot17_train']
        VAL_USE: ['mot15_val']

      Panda:
        TRAIN_USE: ['panda_train']
        VAL_USE: ['panda_val']

    AUG:
      hsv_h: 0.015       # image HSV-Hue augmentation (fraction)
      hsv_s: 0.5         # image HSV-Saturation augmentation (fraction)
      hsv_v: 0.4         # image HSV-Value augmentation (fraction)
      degrees: 0.0       # image rotation (+/- deg)
      translate: 0.0     # image translation (+/- fraction)
      scale: 0.0         # image scale (+/- gain)
      shear: 0.0         # image shear (+/- deg)
      perspective: 0.0   # image perspective (+/- fraction), range 0-0.001
      flipud: 0.0        # image flip up-down (probability)
      fliplr: 0.5        # image flip left-right (probability)
      mixup: 0.0         # image mixup (probability)
      cutout: 0.0        # image cutout

TEST:    # TEST model is same as TRAIN.MODEL
  COMMON_HYPER:         # common hyper-parameters
    nms_thres: 0.6      # threshold for detector nms process
    conf_thres: 0.5     # objectness threshold
    track_buffer: 30    # max saving frames for tracklets
    min_box_area: 100   # min box area
    img_size: !!python/tuple [1088, 608]  # input image size
    mean: [0.408, 0.447, 0.470]  # mean and std to normalize image
    std: [0.289, 0.274, 0.278]
  KITTI:







